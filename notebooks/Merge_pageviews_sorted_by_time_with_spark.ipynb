{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import gzip\n",
    "import glob\n",
    "import pathlib\n",
    "import argparse\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "########## logging\n",
    "# create logger with 'spam_application'\n",
    "logger = logging.getLogger('notebook')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# create console handler with a higher log level\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('[%(asctime)s][%(levelname)s]: %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "# add the handlers to the logger\n",
    "logger.addHandler(ch)\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<progressbar.utils.WrappingIO at 0x7ff7bcd83470>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-19 23:46:57,473][DEBUG]: input_files_count: 10\n",
      "[2018-05-19 23:46:57,498][DEBUG]: input_file: data/test/pagecounts-20071210-070000.gz\n",
      "[2018-05-19 23:46:57,503][INFO]: Processing file: data/test/pagecounts-20071210-070000.gz\n",
      "[2018-05-19 23:47:00,404][INFO]: Added DataFrame for file data/test/pagecounts-20071210-070000.gz to list\n",
      "[2018-05-19 23:47:00,405][DEBUG]: input_file: data/test/pagecounts-20071210-000000.gz\n",
      "[2018-05-19 23:47:00,406][INFO]: Processing file: data/test/pagecounts-20071210-000000.gz\n",
      "[2018-05-19 23:47:00,496][INFO]: Added DataFrame for file data/test/pagecounts-20071210-000000.gz to list\n",
      "[2018-05-19 23:47:00,496][DEBUG]: input_file: data/test/pagecounts-20071210-080000.gz\n",
      "[2018-05-19 23:47:00,498][INFO]: Processing file: data/test/pagecounts-20071210-080000.gz\n",
      "[2018-05-19 23:47:00,562][INFO]: Added DataFrame for file data/test/pagecounts-20071210-080000.gz to list\n",
      "[2018-05-19 23:47:00,567][DEBUG]: input_file: data/test/pagecounts-20071210-020000.gz\n",
      "[2018-05-19 23:47:00,568][INFO]: Processing file: data/test/pagecounts-20071210-020000.gz\n",
      "[2018-05-19 23:47:00,626][INFO]: Added DataFrame for file data/test/pagecounts-20071210-020000.gz to list\n",
      "[2018-05-19 23:47:00,631][DEBUG]: input_file: data/test/pagecounts-20071210-010000.gz\n",
      "[2018-05-19 23:47:00,633][INFO]: Processing file: data/test/pagecounts-20071210-010000.gz\n",
      "[2018-05-19 23:47:00,689][INFO]: Added DataFrame for file data/test/pagecounts-20071210-010000.gz to list\n",
      "[2018-05-19 23:47:00,691][DEBUG]: input_file: data/test/pagecounts-20071210-050000.gz\n",
      "[2018-05-19 23:47:00,693][INFO]: Processing file: data/test/pagecounts-20071210-050000.gz\n",
      "[2018-05-19 23:47:00,809][INFO]: Added DataFrame for file data/test/pagecounts-20071210-050000.gz to list\n",
      "[2018-05-19 23:47:00,811][DEBUG]: input_file: data/test/pagecounts-20071210-060000.gz\n",
      "[2018-05-19 23:47:00,812][INFO]: Processing file: data/test/pagecounts-20071210-060000.gz\n",
      "[2018-05-19 23:47:00,884][INFO]: Added DataFrame for file data/test/pagecounts-20071210-060000.gz to list\n",
      "[2018-05-19 23:47:00,886][DEBUG]: input_file: data/test/pagecounts-20071210-040001.gz\n",
      "[2018-05-19 23:47:00,887][INFO]: Processing file: data/test/pagecounts-20071210-040001.gz\n",
      "[2018-05-19 23:47:00,959][INFO]: Added DataFrame for file data/test/pagecounts-20071210-040001.gz to list\n",
      "[2018-05-19 23:47:00,961][DEBUG]: input_file: data/test/pagecounts-20071210-090000.gz\n",
      "[2018-05-19 23:47:00,962][INFO]: Processing file: data/test/pagecounts-20071210-090000.gz\n",
      "[2018-05-19 23:47:01,016][INFO]: Added DataFrame for file data/test/pagecounts-20071210-090000.gz to list\n",
      "[2018-05-19 23:47:01,018][DEBUG]: input_file: data/test/pagecounts-20071210-030000.gz\n",
      "[2018-05-19 23:47:01,024][INFO]: Processing file: data/test/pagecounts-20071210-030000.gz\n",
      "[2018-05-19 23:47:01,054][INFO]: Added DataFrame for file data/test/pagecounts-20071210-030000.gz to list\n",
      "100% (10 of 10) |########################| Elapsed Time: 0:00:00 ETA:  00:00:00\n",
      "[2018-05-19 23:47:01,080][INFO]: Union of all Spark DataFrames.\n",
      "[2018-05-19 23:47:03,278][INFO]: Spark DataFrame created\n",
      "[2018-05-19 23:47:04,894][INFO]: Dropping column \"reqbytes\" from DataFrame\n",
      "[2018-05-19 23:47:04,911][INFO]: Dropped column \"reqbytes\" from DataFrame\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "progressbar.streams.wrap_stderr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# https://github.com/jupyter/docker-stacks/wiki/Docker-recipes#using-local-spark-jars\n",
    "\n",
    "jarname = 'concatgroup_2.11-2.3.0_0.1.jar'\n",
    "jarpath =  os.path.join(os.getcwd(), jarname)\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars {}'.format(jarpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().set(\"spark.jars\", jarpath)\n",
    "sc = pyspark.SparkContext(appName=\"merge-pagecounts\", conf=conf)\n",
    "sqlctx = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"lang\", StringType(), False),\n",
    "                     StructField(\"page\", StringType(), False),\n",
    "                     StructField(\"views\", IntegerType(), False),\n",
    "                     StructField(\"reqbytes\", IntegerType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(*dfs):\n",
    "    first, *_ = dfs  # Python 3.x, for 2.x you'll have to unpack manually\n",
    "    return first.sql_ctx.createDataFrame(\n",
    "        first.sql_ctx._sc.union([df.rdd for df in dfs]),\n",
    "        first.schema\n",
    "    )\n",
    "\n",
    "\n",
    "def date_parser(timestamp):\n",
    "    return datetime.datetime.strptime(timestamp, '%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathfile = \"data/test/pagecounts-20071210-0*.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files_count = len([f for f in glob.iglob(pathfile)])\n",
    "input_files = glob.iglob(pathfile)\n",
    "\n",
    "# input_files = [\"data/input/sorted_time/2007-12/pagecounts-20071210-000000.gz\",\n",
    "#                \"data/input/sorted_time/2007-12/pagecounts-20071210-010000.gz\"\n",
    "#                ]\n",
    "# input_files_count = len(input_files)\n",
    "\n",
    "logger.debug('input_files_count: {}'.format(input_files_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_files_count < 1:\n",
    "    logger.warn('No input files match: exiting')\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_files_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dfs = list()\n",
    "with progressbar.ProgressBar(max_value=input_files_count) as bar:\n",
    "    for input_file in input_files:\n",
    "        logger.debug('input_file: {}'.format(input_file))\n",
    "\n",
    "        timestamp = date_parser(os.path.basename(input_file)\n",
    "                                       .replace('pagecounts-','')\n",
    "                                       .replace('.gz',''))\n",
    "\n",
    "        logger.info('Processing file: {}'.format(input_file))\n",
    "        tmp_spark_df = sqlctx.read.csv(\n",
    "                            input_file,\n",
    "                            header=False,\n",
    "                            schema=schema,\n",
    "                            sep=' ')\n",
    "\n",
    "        tmp_spark_df = tmp_spark_df.withColumn(\"timestamp\", lit(timestamp))\n",
    "        list_dfs.append(tmp_spark_df)\n",
    "        del tmp_spark_df\n",
    "\n",
    "        logger.info('Added DataFrame for file {} to list'.format(input_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(list_dfs) >= 1, 'There should be at least one DataFrame'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(list_dfs) > 1:\n",
    "    logger.info('Union of all Spark DataFrames.')\n",
    "    df = unionAll(*list_dfs)\n",
    "    logger.info('Spark DataFrame created')\n",
    "else:\n",
    "    df = list_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Dropping column \"reqbytes\" from DataFrame')\n",
    "df = df.drop('reqbytes')\n",
    "logger.info('Dropped column \"reqbytes\" from DataFrame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lang', 'string'),\n",
       " ('page', 'string'),\n",
       " ('views', 'int'),\n",
       " ('timestamp', 'timestamp')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import BooleanType\n",
    "# sqlctx.udf.registerJavaFunction(\"group_concat\", \"org.wikipedia.spark.udf.GroupConcat\", BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_schema = StructType([StructField(\"lang\", StringType(), False),\n",
    "                         StructField(\"page\", StringType(), False),\n",
    "                         StructField(\"day\", StringType(), False),\n",
    "                         StructField(\"enc\", StringType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @pandas_udf(schema, functionType=PandasUDFType.GROUP_MAP)\n",
    "# def g(df):\n",
    "#     result = pd.DataFrame(df.groupby(df.key).apply(\n",
    "#         lambda x: x.loc[:, [\"value1\", \"value2\"]].min(axis=1).mean()\n",
    "#     ))\n",
    "#     result.reset_index(inplace=True, drop=False)\n",
    "#     return result\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pandas as pd\n",
    "\n",
    "hour_to_letter = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O',\n",
    "                  'P','Q','R','S','T','U','V','W','X']\n",
    "\n",
    "@pandas_udf(new_schema, PandasUDFType.GROUPED_MAP)\n",
    "def concat_hours(x):\n",
    "    view_hours = x['hour'].tolist()\n",
    "    view_views = x['views'].tolist()\n",
    "\n",
    "    view_hours_letters = [hour_to_letter[h] for h in view_hours]\n",
    "\n",
    "    encoded_views = [l + str(h)\n",
    "                     for l, h in sorted(zip(view_hours_letters,view_views))]\n",
    "    encoded_views_string = ''.join(encoded_views)\n",
    "\n",
    "    # return pd.DataFrame({'page': x.page, 'lang': x.lang,'day': x.day, 'enc': encoded_views_string}, index=[x.index[0]])\n",
    "    return pd.DataFrame({'enc': x.page, 'day': x.lang, 'lang': x.day, 'page': encoded_views_string}, index=[x.index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grouped_df = (df.select(['lang',\n",
    "#                          'page',\n",
    "#                          functions.date_format('timestamp','yyyy-MM-dd').alias('day'), \n",
    "#                          functions.hour('timestamp').alias('hour'), \n",
    "#                          'views'\n",
    "#                          ])\n",
    "#                 .groupby(['lang','page','day'])\n",
    "#                 .apply(concat_hours)\n",
    "#                 .dropDuplicates()\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions\n",
    "grouped_df = (df.select(['lang',\n",
    "                         'page',\n",
    "                         functions.date_format('timestamp','yyyy-MM-dd').alias('day'), \n",
    "                         functions.hour('timestamp').alias('hour'), \n",
    "                         'views'\n",
    "                         ])\n",
    "                .groupby(['lang','page','day'])\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = (grouped_df.apply(concat_hours)\n",
    "                        .dropDuplicates()\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+--------------------+\n",
      "|lang|                page|       day|                 enc|\n",
      "+----+--------------------+----------+--------------------+\n",
      "|  en|        Albert_Cahen|2007-12-10|                  J1|\n",
      "|  en|     Albert_Campbell|2007-12-10|            A1D1F1I1|\n",
      "|  en|Albert_C._L._G._G...|2007-12-10|                  J1|\n",
      "|  en|Albert_Cardinal_V...|2007-12-10|                  E1|\n",
      "|  en|        Albert_Canal|2007-12-10|            A2C1G2I1|\n",
      "|  en|Albert_Camus#Oppo...|2007-12-10|                  E1|\n",
      "|  en|       Albert_Brooks|2007-12-10|                 G16|\n",
      "|  en|      Albert_Cashier|2007-12-10|          A2B1D7E1H2|\n",
      "|  en|     Albert_Castillo|2007-12-10|              B1E1H1|\n",
      "|  en|    Albert_C._Outler|2007-12-10|                  H2|\n",
      "|  en|   Albert_Carotenuto|2007-12-10|                D1H1|\n",
      "|  en|     Albert_C._Field|2007-12-10|                  G1|\n",
      "|  en|Albert_Camus#Summ...|2007-12-10|                  D1|\n",
      "|  en|       Albert_Cavens|2007-12-10|                  A1|\n",
      "|  en|        Albert_Camus|2007-12-10|A150B148C197D173E...|\n",
      "|  en|Albert_CamusNon-f...|2007-12-10|                  I1|\n",
      "|  en|    Albert_Carnesale|2007-12-10|                G4H5|\n",
      "|  en|      Albert_Cadwell|2007-12-10|                  D1|\n",
      "|  en|     Albert_Calmette|2007-12-10|            B1C1F1H1|\n",
      "|  en|       Albert_C_Read|2007-12-10|                  I1|\n",
      "+----+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "result = df.select([col('lang').alias('result_lang'),\n",
    "                    col('page').alias('result_page'),\n",
    "                    functions.date_format('timestamp','yyyy-MM-dd').alias('result_day')\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+\n",
      "|result_lang|         result_page|result_day|\n",
      "+-----------+--------------------+----------+\n",
      "|         en|Albert_Bushnell_Hart|2007-12-10|\n",
      "|         en|    Albert_C._Outler|2007-12-10|\n",
      "|         en|     Albert_Calmette|2007-12-10|\n",
      "|         en|Albert_Campbell_S...|2007-12-10|\n",
      "|         en|      Albert_Campion|2007-12-10|\n",
      "|         en|        Albert_Camus|2007-12-10|\n",
      "|         en|Albert_Camus/the_...|2007-12-10|\n",
      "|         en|      Albert_Cardozo|2007-12-10|\n",
      "|         en|    Albert_Carnesale|2007-12-10|\n",
      "|         en|   Albert_Carotenuto|2007-12-10|\n",
      "|         en|      Albert_Cashier|2007-12-10|\n",
      "|         en|     Albert_Castillo|2007-12-10|\n",
      "|         en|      Albert_Bunjaku|2007-12-10|\n",
      "|         en|Albert_C._L._G._G...|2007-12-10|\n",
      "|         en|    Albert_C._Vaughn|2007-12-10|\n",
      "|         en|     Albert_Campbell|2007-12-10|\n",
      "|         en|Albert_Campbell_C...|2007-12-10|\n",
      "|         en|        Albert_Camus|2007-12-10|\n",
      "|         en|        Albert_Canal|2007-12-10|\n",
      "|         en|      Albert_Cardozo|2007-12-10|\n",
      "+-----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = [result.result_lang == grouped_df.lang, result.result_page == grouped_df.page, result.result_day == grouped_df.day]\n",
    "final = result.join(grouped_df, cond).select(['result_lang','result_page','result_day','enc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+--------------------+\n",
      "|result_lang|         result_page|result_day|                 enc|\n",
      "+-----------+--------------------+----------+--------------------+\n",
      "|         en|Albert_Camus#Furt...|2007-12-10|                  E1|\n",
      "|         en|Albert_Camuscolum...|2007-12-10|                  J1|\n",
      "|         en|    Albert_Carnesale|2007-12-10|                G4H5|\n",
      "|         en|      Albert_Bunjaku|2007-12-10|                  A1|\n",
      "|         en|        Albert_Camus|2007-12-10|A150B148C197D173E...|\n",
      "|         en|      Albert_Cadwell|2007-12-10|                  D1|\n",
      "|         en|     Albert_C._Field|2007-12-10|                  G1|\n",
      "|         en|Albert_Camus#Oppo...|2007-12-10|                  E1|\n",
      "|         en|       Albert_Caquot|2007-12-10|            C1E1G1I1|\n",
      "|         en|Albert_CamusNon-f...|2007-12-10|                  I1|\n",
      "|         en|      Albert_Celades|2007-12-10|              A3B1J2|\n",
      "|         en|      Albert_Cashier|2007-12-10|          A2B1D7E1H2|\n",
      "|         en|      Albert_Calland|2007-12-10|              B1D3I1|\n",
      "|         en|       Albert_C_Read|2007-12-10|                  I1|\n",
      "|         en|   Albert_Carotenuto|2007-12-10|                D1H1|\n",
      "|         en|    Albert_C._Greene|2007-12-10|              C1D1E1|\n",
      "|         en|Albert_Campbell%2...|2007-12-10|              B1F1I1|\n",
      "|         en| Albert_C._Wedemeyer|2007-12-10|                  B1|\n",
      "|         en|Albert_Charles_Sa...|2007-12-10|                  D1|\n",
      "|         en|    Albert_Caravello|2007-12-10|              C1F1I1|\n",
      "+-----------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
