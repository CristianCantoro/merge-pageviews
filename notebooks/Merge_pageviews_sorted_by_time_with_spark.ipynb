{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import gzip\n",
    "import glob\n",
    "import pathlib\n",
    "import argparse\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "########## logging\n",
    "# create logger with 'spam_application'\n",
    "logger = logging.getLogger('notebook')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# create console handler with a higher log level\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('[%(asctime)s][%(levelname)s]: %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "# add the handlers to the logger\n",
    "logger.addHandler(ch)\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<progressbar.utils.WrappingIO at 0x7fd5b06ac278>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-23 09:41:49,434][DEBUG]: input_files_count: 24\n",
      "[2018-05-23 09:41:49,462][DEBUG]: input_file: ../data/test/pagecounts-20071210-070000.gz\n",
      "[2018-05-23 09:41:49,465][INFO]: Processing file: ../data/test/pagecounts-20071210-070000.gz\n",
      "[2018-05-23 09:41:52,008][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-070000.gz to list\n",
      "[2018-05-23 09:41:52,009][DEBUG]: input_file: ../data/test/pagecounts-20071210-200000.gz\n",
      "[2018-05-23 09:41:52,009][INFO]: Processing file: ../data/test/pagecounts-20071210-200000.gz\n",
      "[2018-05-23 09:41:52,045][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-200000.gz to list\n",
      "[2018-05-23 09:41:52,049][DEBUG]: input_file: ../data/test/pagecounts-20071210-000000.gz\n",
      "[2018-05-23 09:41:52,051][INFO]: Processing file: ../data/test/pagecounts-20071210-000000.gz\n",
      "[2018-05-23 09:41:52,139][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-000000.gz to list\n",
      "[2018-05-23 09:41:52,146][DEBUG]: input_file: ../data/test/pagecounts-20071210-130000.gz\n",
      "[2018-05-23 09:41:52,150][INFO]: Processing file: ../data/test/pagecounts-20071210-130000.gz\n",
      "[2018-05-23 09:41:52,261][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-130000.gz to list\n",
      "[2018-05-23 09:41:52,264][DEBUG]: input_file: ../data/test/pagecounts-20071210-160001.gz\n",
      "[2018-05-23 09:41:52,266][INFO]: Processing file: ../data/test/pagecounts-20071210-160001.gz\n",
      "[2018-05-23 09:41:52,339][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-160001.gz to list\n",
      "[2018-05-23 09:41:52,342][DEBUG]: input_file: ../data/test/pagecounts-20071210-190000.gz\n",
      "[2018-05-23 09:41:52,343][INFO]: Processing file: ../data/test/pagecounts-20071210-190000.gz\n",
      "[2018-05-23 09:41:52,410][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-190000.gz to list\n",
      "[2018-05-23 09:41:52,412][DEBUG]: input_file: ../data/test/pagecounts-20071210-080000.gz\n",
      "[2018-05-23 09:41:52,415][INFO]: Processing file: ../data/test/pagecounts-20071210-080000.gz\n",
      "[2018-05-23 09:41:52,467][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-080000.gz to list\n",
      "[2018-05-23 09:41:52,470][DEBUG]: input_file: ../data/test/pagecounts-20071210-140000.gz\n",
      "[2018-05-23 09:41:52,471][INFO]: Processing file: ../data/test/pagecounts-20071210-140000.gz\n",
      "[2018-05-23 09:41:52,515][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-140000.gz to list\n",
      "[2018-05-23 09:41:52,518][DEBUG]: input_file: ../data/test/pagecounts-20071210-110000.gz\n",
      "[2018-05-23 09:41:52,519][INFO]: Processing file: ../data/test/pagecounts-20071210-110000.gz\n",
      "[2018-05-23 09:41:52,550][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-110000.gz to list\n",
      "[2018-05-23 09:41:52,551][DEBUG]: input_file: ../data/test/pagecounts-20071210-020000.gz\n",
      "[2018-05-23 09:41:52,560][INFO]: Processing file: ../data/test/pagecounts-20071210-020000.gz\n",
      "[2018-05-23 09:41:52,589][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-020000.gz to list\n",
      "[2018-05-23 09:41:52,591][DEBUG]: input_file: ../data/test/pagecounts-20071210-010000.gz\n",
      "[2018-05-23 09:41:52,592][INFO]: Processing file: ../data/test/pagecounts-20071210-010000.gz\n",
      "[2018-05-23 09:41:52,620][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-010000.gz to list\n",
      "[2018-05-23 09:41:52,623][DEBUG]: input_file: ../data/test/pagecounts-20071210-220001.gz\n",
      "[2018-05-23 09:41:52,624][INFO]: Processing file: ../data/test/pagecounts-20071210-220001.gz\n",
      "[2018-05-23 09:41:52,670][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-220001.gz to list\n",
      "[2018-05-23 09:41:52,671][DEBUG]: input_file: ../data/test/pagecounts-20071210-180000.gz\n",
      "[2018-05-23 09:41:52,672][INFO]: Processing file: ../data/test/pagecounts-20071210-180000.gz\n",
      "[2018-05-23 09:41:52,698][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-180000.gz to list\n",
      "[2018-05-23 09:41:52,699][DEBUG]: input_file: ../data/test/pagecounts-20071210-050000.gz\n",
      "[2018-05-23 09:41:52,700][INFO]: Processing file: ../data/test/pagecounts-20071210-050000.gz\n",
      "[2018-05-23 09:41:52,730][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-050000.gz to list\n",
      "[2018-05-23 09:41:52,731][DEBUG]: input_file: ../data/test/pagecounts-20071210-060000.gz\n",
      "[2018-05-23 09:41:52,731][INFO]: Processing file: ../data/test/pagecounts-20071210-060000.gz\n",
      "[2018-05-23 09:41:52,770][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-060000.gz to list\n",
      "[2018-05-23 09:41:52,771][DEBUG]: input_file: ../data/test/pagecounts-20071210-170000.gz\n",
      "[2018-05-23 09:41:52,771][INFO]: Processing file: ../data/test/pagecounts-20071210-170000.gz\n",
      "[2018-05-23 09:41:52,818][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-170000.gz to list\n",
      "[2018-05-23 09:41:52,823][DEBUG]: input_file: ../data/test/pagecounts-20071210-230000.gz\n",
      "[2018-05-23 09:41:52,825][INFO]: Processing file: ../data/test/pagecounts-20071210-230000.gz\n",
      "[2018-05-23 09:41:52,860][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-230000.gz to list\n",
      "[2018-05-23 09:41:52,874][DEBUG]: input_file: ../data/test/pagecounts-20071210-150000.gz\n",
      "[2018-05-23 09:41:52,875][INFO]: Processing file: ../data/test/pagecounts-20071210-150000.gz\n",
      "[2018-05-23 09:41:52,910][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-150000.gz to list\n",
      "[2018-05-23 09:41:52,912][DEBUG]: input_file: ../data/test/pagecounts-20071210-040001.gz\n",
      "[2018-05-23 09:41:52,913][INFO]: Processing file: ../data/test/pagecounts-20071210-040001.gz\n",
      "[2018-05-23 09:41:52,997][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-040001.gz to list\n",
      "[2018-05-23 09:41:52,998][DEBUG]: input_file: ../data/test/pagecounts-20071210-120000.gz\n",
      "[2018-05-23 09:41:53,000][INFO]: Processing file: ../data/test/pagecounts-20071210-120000.gz\n",
      "[2018-05-23 09:41:53,043][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-120000.gz to list\n",
      "[2018-05-23 09:41:53,045][DEBUG]: input_file: ../data/test/pagecounts-20071210-090000.gz\n",
      "[2018-05-23 09:41:53,046][INFO]: Processing file: ../data/test/pagecounts-20071210-090000.gz\n",
      "[2018-05-23 09:41:53,087][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-090000.gz to list\n",
      "[2018-05-23 09:41:53,087][DEBUG]: input_file: ../data/test/pagecounts-20071210-030000.gz\n",
      "[2018-05-23 09:41:53,089][INFO]: Processing file: ../data/test/pagecounts-20071210-030000.gz\n",
      "[2018-05-23 09:41:53,129][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-030000.gz to list\n",
      "[2018-05-23 09:41:53,130][DEBUG]: input_file: ../data/test/pagecounts-20071210-100001.gz\n",
      "[2018-05-23 09:41:53,131][INFO]: Processing file: ../data/test/pagecounts-20071210-100001.gz\n",
      "[2018-05-23 09:41:53,159][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-100001.gz to list\n",
      "[2018-05-23 09:41:53,160][DEBUG]: input_file: ../data/test/pagecounts-20071210-210000.gz\n",
      "[2018-05-23 09:41:53,161][INFO]: Processing file: ../data/test/pagecounts-20071210-210000.gz\n",
      "[2018-05-23 09:41:53,209][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-210000.gz to list\n",
      "100% (24 of 24) |########################| Elapsed Time: 0:00:00 ETA:  00:00:00\n",
      "[2018-05-23 09:41:53,240][INFO]: Union of all Spark DataFrames.\n",
      "[2018-05-23 09:41:55,335][INFO]: Spark DataFrame created\n",
      "[2018-05-23 09:41:57,190][INFO]: Dropping column \"reqbytes\" from DataFrame\n",
      "[2018-05-23 09:41:57,214][INFO]: Dropped column \"reqbytes\" from DataFrame\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "progressbar.streams.wrap_stderr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"merge-pagecounts\")\n",
    "sqlctx = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"lang\", StringType(), False),\n",
    "                     StructField(\"page\", StringType(), False),\n",
    "                     StructField(\"views\", IntegerType(), False),\n",
    "                     StructField(\"reqbytes\", IntegerType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(*dfs):\n",
    "    first, *_ = dfs  # Python 3.x, for 2.x you'll have to unpack manually\n",
    "    return first.sql_ctx.createDataFrame(\n",
    "        first.sql_ctx._sc.union([df.rdd for df in dfs]),\n",
    "        first.schema\n",
    "    )\n",
    "\n",
    "\n",
    "def date_parser(timestamp):\n",
    "    return datetime.datetime.strptime(timestamp, '%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathfile = \"../data/test/pagecounts-20071210-*.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files_count = len([f for f in glob.iglob(pathfile)])\n",
    "input_files = glob.iglob(pathfile)\n",
    "\n",
    "# input_files = [\"data/input/sorted_time/2007-12/pagecounts-20071210-000000.gz\",\n",
    "#                \"data/input/sorted_time/2007-12/pagecounts-20071210-010000.gz\"\n",
    "#                ]\n",
    "# input_files_count = len(input_files)\n",
    "\n",
    "logger.debug('input_files_count: {}'.format(input_files_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_files_count < 1:\n",
    "    logger.warn('No input files match: exiting')\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_files_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dfs = list()\n",
    "with progressbar.ProgressBar(max_value=input_files_count) as bar:\n",
    "    for input_file in input_files:\n",
    "        logger.debug('input_file: {}'.format(input_file))\n",
    "\n",
    "        timestamp = date_parser(os.path.basename(input_file)\n",
    "                                       .replace('pagecounts-','')\n",
    "                                       .replace('.gz',''))\n",
    "\n",
    "        logger.info('Processing file: {}'.format(input_file))\n",
    "        tmp_spark_df = sqlctx.read.csv(\n",
    "                            input_file,\n",
    "                            header=False,\n",
    "                            schema=schema,\n",
    "                            sep=' ')\n",
    "\n",
    "        tmp_spark_df = tmp_spark_df.withColumn(\"timestamp\", lit(timestamp))\n",
    "        list_dfs.append(tmp_spark_df)\n",
    "        del tmp_spark_df\n",
    "\n",
    "        logger.info('Added DataFrame for file {} to list'.format(input_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(list_dfs) >= 1, 'There should be at least one DataFrame'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(list_dfs) > 1:\n",
    "    logger.info('Union of all Spark DataFrames.')\n",
    "    df = unionAll(*list_dfs)\n",
    "    logger.info('Spark DataFrame created')\n",
    "else:\n",
    "    df = list_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Dropping column \"reqbytes\" from DataFrame')\n",
    "df = df.drop('reqbytes')\n",
    "logger.info('Dropped column \"reqbytes\" from DataFrame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lang', 'string'),\n",
       " ('page', 'string'),\n",
       " ('views', 'int'),\n",
       " ('timestamp', 'timestamp')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_daily_df = (df.select(['lang',\n",
    "                               'page',\n",
    "                               functions.date_format('timestamp','yyyy-MM-dd')\\\n",
    "                                        .alias('day'),\n",
    "                               'views'])\n",
    "                      .groupby(['lang','page','day'])\n",
    "                      .sum('views')\n",
    "                      .dropDuplicates()\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "grouped_daily_df = grouped_daily_df.select([col('lang').alias('lang_d'),\n",
    "                                            col('page').alias('page_d'),\n",
    "                                            col('day').alias('day_d'),\n",
    "                                            col('sum(views)').alias('daily_views'),                                 \n",
    "                                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+-----------+\n",
      "|lang_d|              page_d|     day_d|daily_views|\n",
      "+------+--------------------+----------+-----------+\n",
      "|    en|Albert_Cardinal_D...|2007-12-10|          1|\n",
      "|    en| Albert_Caesar_Tocco|2007-12-10|          2|\n",
      "|    en|    Albert_C__Outler|2007-12-10|          1|\n",
      "|    en|Albert_Camus#Furt...|2007-12-10|          1|\n",
      "|    en|Albert_Camuscolum...|2007-12-10|          1|\n",
      "|    en|    Albert_Carnesale|2007-12-10|         12|\n",
      "|    en|        Albert_Buick|2007-12-10|          1|\n",
      "|    en|        Albert_Camus|2007-12-10|       2908|\n",
      "|    en|      Albert_Bunjaku|2007-12-10|          1|\n",
      "|    en|        Albert_Burgh|2007-12-10|          1|\n",
      "+------+--------------------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_daily_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_schema = StructType([StructField(\"lang\", StringType(), False),\n",
    "                         StructField(\"page\", StringType(), False),\n",
    "                         StructField(\"day\", StringType(), False),\n",
    "                         StructField(\"enc\", StringType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pandas as pd\n",
    "\n",
    "hour_to_letter = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O',\n",
    "                  'P','Q','R','S','T','U','V','W','X']\n",
    "\n",
    "@pandas_udf(new_schema, PandasUDFType.GROUPED_MAP)\n",
    "def concat_hours(x):\n",
    "    view_hours = x['hour'].tolist()\n",
    "    view_views = x['views'].tolist()\n",
    "\n",
    "    view_hours_letters = [hour_to_letter[h] for h in view_hours]\n",
    "\n",
    "    encoded_views = [l + str(h)\n",
    "                     for l, h in sorted(zip(view_hours_letters,view_views))]\n",
    "    encoded_views_string = ''.join(encoded_views)\n",
    "\n",
    "    # return pd.DataFrame({'page': x.page, 'lang': x.lang,'day': x.day, 'enc': encoded_views_string}, index=[x.index[0]])\n",
    "    return pd.DataFrame({'enc': x.page, 'day': x.lang, 'lang': x.day, 'page': encoded_views_string}, index=[x.index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions\n",
    "grouped_hours_df = (df.select(['lang',\n",
    "                               'page',\n",
    "                               functions.date_format('timestamp','yyyy-MM-dd').alias('day'), \n",
    "                               functions.hour('timestamp').alias('hour'), \n",
    "                               'views'\n",
    "                               ])\n",
    "                      .groupby(['lang','page','day'])\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_hours_df = (grouped_hours_df.apply(concat_hours)\n",
    "                                    .dropDuplicates()\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+--------------------+\n",
      "|lang|                page|       day|                 enc|\n",
      "+----+--------------------+----------+--------------------+\n",
      "|  en|   Albert_C._Ritchie|2007-12-10|            Q2S2V1W1|\n",
      "|  en|Albert_C._L._G._G...|2007-12-10|                  J1|\n",
      "|  en|      Albert_Calland|2007-12-10|        B1D3I1L1O1Q1|\n",
      "|  en|       Albert_Brooks|2007-12-10|           G16K11N13|\n",
      "|  en|    Albert_C1984amus|2007-12-10|                  V1|\n",
      "|  en| Albert_C._Wedemeyer|2007-12-10|                B1X1|\n",
      "|  en|       Albert_Chowne|2007-12-10|                  R1|\n",
      "|  en|        Albert_Camus|2007-12-10|A150B148C197D173E...|\n",
      "|  en|Albert_Cardinal_V...|2007-12-10|                  E1|\n",
      "|  en|    Albert_C__Barnes|2007-12-10|                  U1|\n",
      "+----+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_hours_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+-----------+\n",
      "|lang_d|              page_d|     day_d|daily_views|\n",
      "+------+--------------------+----------+-----------+\n",
      "|    en|Albert_Cardinal_D...|2007-12-10|          1|\n",
      "|    en| Albert_Caesar_Tocco|2007-12-10|          2|\n",
      "|    en|    Albert_C__Outler|2007-12-10|          1|\n",
      "|    en|Albert_Camus#Furt...|2007-12-10|          1|\n",
      "|    en|Albert_Camuscolum...|2007-12-10|          1|\n",
      "|    en|    Albert_Carnesale|2007-12-10|         12|\n",
      "|    en|        Albert_Buick|2007-12-10|          1|\n",
      "|    en|        Albert_Camus|2007-12-10|       2908|\n",
      "|    en|      Albert_Bunjaku|2007-12-10|          1|\n",
      "|    en|        Albert_Burgh|2007-12-10|          1|\n",
      "+------+--------------------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_daily_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = [grouped_daily_df.lang_d == grouped_hours_df.lang,\n",
    "        grouped_daily_df.page_d == grouped_hours_df.page,\n",
    "        grouped_daily_df.day_d == grouped_hours_df.day]\n",
    "final = (grouped_daily_df.join(grouped_hours_df, cond)\n",
    "                         .select('lang_d', 'page_d', 'day_d', 'daily_views', 'enc')\n",
    "                         .dropDuplicates()\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+-----------+--------------------+\n",
      "|lang_d|              page_d|     day_d|daily_views|                 enc|\n",
      "+------+--------------------+----------+-----------+--------------------+\n",
      "|    en|Albert_Cardinal_D...|2007-12-10|          1|                  Q1|\n",
      "|    en|    Albert_C__Outler|2007-12-10|          1|                  R1|\n",
      "|    en| Albert_Caesar_Tocco|2007-12-10|          2|                N1P1|\n",
      "|    en|Albert_Camus#Furt...|2007-12-10|          1|                  E1|\n",
      "|    en|Albert_Camuscolum...|2007-12-10|          1|                  J1|\n",
      "|    en|    Albert_Carnesale|2007-12-10|         12|          G4H5L1V1X1|\n",
      "|    en|        Albert_Buick|2007-12-10|          1|                  K1|\n",
      "|    en|      Albert_Bunjaku|2007-12-10|          1|                  A1|\n",
      "|    en|        Albert_Burgh|2007-12-10|          1|                  M1|\n",
      "|    en|        Albert_Camus|2007-12-10|       2908|A150B148C197D173E...|\n",
      "|    en|      Albert_Cadwell|2007-12-10|          1|                  D1|\n",
      "|    en|     Albert_C._Field|2007-12-10|          2|                G1O1|\n",
      "|    en|Albert_Camus#Oppo...|2007-12-10|          1|                  E1|\n",
      "|    en|       Albert_Caquot|2007-12-10|          8|        C1E1G1I1P3V1|\n",
      "|    en|         Albert_Chen|2007-12-10|          1|                  N1|\n",
      "|    en|Albert_CamusNon-f...|2007-12-10|          1|                  I1|\n",
      "|    en|   Albert_Brudzewski|2007-12-10|          2|                  M2|\n",
      "|    en|      Albert_Celades|2007-12-10|         19|A3B1J2K1N2P2Q1S2T...|\n",
      "|    en|      Albert_Cashier|2007-12-10|         40|A2B1D7E1H2K1L2M1O...|\n",
      "|    en|      Albert_Burbank|2007-12-10|          1|                  Q1|\n",
      "+------+--------------------+----------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
