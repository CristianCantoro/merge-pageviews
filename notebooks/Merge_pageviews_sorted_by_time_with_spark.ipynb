{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import gzip\n",
    "import glob\n",
    "import pathlib\n",
    "import argparse\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "########## logging\n",
    "# create logger with 'spam_application'\n",
    "logger = logging.getLogger('notebook')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# create console handler with a higher log level\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('[%(asctime)s][%(levelname)s]: %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "# add the handlers to the logger\n",
    "logger.addHandler(ch)\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<progressbar.utils.WrappingIO at 0x7fc2c4210320>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-22 23:04:51,415][DEBUG]: input_files_count: 24\n",
      "[2018-05-22 23:04:51,471][DEBUG]: input_file: ../data/test/pagecounts-20071210-070000.gz\n",
      "[2018-05-22 23:04:51,474][INFO]: Processing file: ../data/test/pagecounts-20071210-070000.gz\n",
      "[2018-05-22 23:04:53,553][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-070000.gz to list\n",
      "[2018-05-22 23:04:53,553][DEBUG]: input_file: ../data/test/pagecounts-20071210-200000.gz\n",
      "[2018-05-22 23:04:53,555][INFO]: Processing file: ../data/test/pagecounts-20071210-200000.gz\n",
      "[2018-05-22 23:04:53,652][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-200000.gz to list\n",
      "[2018-05-22 23:04:53,653][DEBUG]: input_file: ../data/test/pagecounts-20071210-000000.gz\n",
      "[2018-05-22 23:04:53,654][INFO]: Processing file: ../data/test/pagecounts-20071210-000000.gz\n",
      "[2018-05-22 23:04:53,704][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-000000.gz to list\n",
      "[2018-05-22 23:04:53,706][DEBUG]: input_file: ../data/test/pagecounts-20071210-130000.gz\n",
      "[2018-05-22 23:04:53,707][INFO]: Processing file: ../data/test/pagecounts-20071210-130000.gz\n",
      "[2018-05-22 23:04:53,790][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-130000.gz to list\n",
      "[2018-05-22 23:04:53,794][DEBUG]: input_file: ../data/test/pagecounts-20071210-160001.gz\n",
      "[2018-05-22 23:04:53,797][INFO]: Processing file: ../data/test/pagecounts-20071210-160001.gz\n",
      "[2018-05-22 23:04:53,851][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-160001.gz to list\n",
      "[2018-05-22 23:04:53,854][DEBUG]: input_file: ../data/test/pagecounts-20071210-190000.gz\n",
      "[2018-05-22 23:04:53,855][INFO]: Processing file: ../data/test/pagecounts-20071210-190000.gz\n",
      "[2018-05-22 23:04:53,903][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-190000.gz to list\n",
      "[2018-05-22 23:04:53,904][DEBUG]: input_file: ../data/test/pagecounts-20071210-080000.gz\n",
      "[2018-05-22 23:04:53,905][INFO]: Processing file: ../data/test/pagecounts-20071210-080000.gz\n",
      "[2018-05-22 23:04:53,946][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-080000.gz to list\n",
      "[2018-05-22 23:04:53,949][DEBUG]: input_file: ../data/test/pagecounts-20071210-140000.gz\n",
      "[2018-05-22 23:04:53,951][INFO]: Processing file: ../data/test/pagecounts-20071210-140000.gz\n",
      "[2018-05-22 23:04:53,991][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-140000.gz to list\n",
      "[2018-05-22 23:04:53,995][DEBUG]: input_file: ../data/test/pagecounts-20071210-110000.gz\n",
      "[2018-05-22 23:04:53,997][INFO]: Processing file: ../data/test/pagecounts-20071210-110000.gz\n",
      "[2018-05-22 23:04:54,027][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-110000.gz to list\n",
      "[2018-05-22 23:04:54,034][DEBUG]: input_file: ../data/test/pagecounts-20071210-020000.gz\n",
      "[2018-05-22 23:04:54,035][INFO]: Processing file: ../data/test/pagecounts-20071210-020000.gz\n",
      "[2018-05-22 23:04:54,069][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-020000.gz to list\n",
      "[2018-05-22 23:04:54,070][DEBUG]: input_file: ../data/test/pagecounts-20071210-010000.gz\n",
      "[2018-05-22 23:04:54,070][INFO]: Processing file: ../data/test/pagecounts-20071210-010000.gz\n",
      "[2018-05-22 23:04:54,114][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-010000.gz to list\n",
      "[2018-05-22 23:04:54,116][DEBUG]: input_file: ../data/test/pagecounts-20071210-220001.gz\n",
      "[2018-05-22 23:04:54,118][INFO]: Processing file: ../data/test/pagecounts-20071210-220001.gz\n",
      "[2018-05-22 23:04:54,152][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-220001.gz to list\n",
      "[2018-05-22 23:04:54,154][DEBUG]: input_file: ../data/test/pagecounts-20071210-180000.gz\n",
      "[2018-05-22 23:04:54,155][INFO]: Processing file: ../data/test/pagecounts-20071210-180000.gz\n",
      "[2018-05-22 23:04:54,192][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-180000.gz to list\n",
      "[2018-05-22 23:04:54,194][DEBUG]: input_file: ../data/test/pagecounts-20071210-050000.gz\n",
      "[2018-05-22 23:04:54,195][INFO]: Processing file: ../data/test/pagecounts-20071210-050000.gz\n",
      "[2018-05-22 23:04:54,236][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-050000.gz to list\n",
      "[2018-05-22 23:04:54,237][DEBUG]: input_file: ../data/test/pagecounts-20071210-060000.gz\n",
      "[2018-05-22 23:04:54,238][INFO]: Processing file: ../data/test/pagecounts-20071210-060000.gz\n",
      "[2018-05-22 23:04:54,273][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-060000.gz to list\n",
      "[2018-05-22 23:04:54,274][DEBUG]: input_file: ../data/test/pagecounts-20071210-170000.gz\n",
      "[2018-05-22 23:04:54,275][INFO]: Processing file: ../data/test/pagecounts-20071210-170000.gz\n",
      "[2018-05-22 23:04:54,322][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-170000.gz to list\n",
      "[2018-05-22 23:04:54,324][DEBUG]: input_file: ../data/test/pagecounts-20071210-230000.gz\n",
      "[2018-05-22 23:04:54,325][INFO]: Processing file: ../data/test/pagecounts-20071210-230000.gz\n",
      "[2018-05-22 23:04:54,358][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-230000.gz to list\n",
      "[2018-05-22 23:04:54,359][DEBUG]: input_file: ../data/test/pagecounts-20071210-150000.gz\n",
      "[2018-05-22 23:04:54,370][INFO]: Processing file: ../data/test/pagecounts-20071210-150000.gz\n",
      "[2018-05-22 23:04:54,402][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-150000.gz to list\n",
      "[2018-05-22 23:04:54,404][DEBUG]: input_file: ../data/test/pagecounts-20071210-040001.gz\n",
      "[2018-05-22 23:04:54,404][INFO]: Processing file: ../data/test/pagecounts-20071210-040001.gz\n",
      "[2018-05-22 23:04:54,453][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-040001.gz to list\n",
      "[2018-05-22 23:04:54,454][DEBUG]: input_file: ../data/test/pagecounts-20071210-120000.gz\n",
      "[2018-05-22 23:04:54,455][INFO]: Processing file: ../data/test/pagecounts-20071210-120000.gz\n",
      "[2018-05-22 23:04:54,509][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-120000.gz to list\n",
      "[2018-05-22 23:04:54,514][DEBUG]: input_file: ../data/test/pagecounts-20071210-090000.gz\n",
      "[2018-05-22 23:04:54,516][INFO]: Processing file: ../data/test/pagecounts-20071210-090000.gz\n",
      "[2018-05-22 23:04:54,570][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-090000.gz to list\n",
      "[2018-05-22 23:04:54,571][DEBUG]: input_file: ../data/test/pagecounts-20071210-030000.gz\n",
      "[2018-05-22 23:04:54,572][INFO]: Processing file: ../data/test/pagecounts-20071210-030000.gz\n",
      "[2018-05-22 23:04:54,623][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-030000.gz to list\n",
      "[2018-05-22 23:04:54,624][DEBUG]: input_file: ../data/test/pagecounts-20071210-100001.gz\n",
      "[2018-05-22 23:04:54,625][INFO]: Processing file: ../data/test/pagecounts-20071210-100001.gz\n",
      "[2018-05-22 23:04:54,646][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-100001.gz to list\n",
      "[2018-05-22 23:04:54,647][DEBUG]: input_file: ../data/test/pagecounts-20071210-210000.gz\n",
      "[2018-05-22 23:04:54,648][INFO]: Processing file: ../data/test/pagecounts-20071210-210000.gz\n",
      "[2018-05-22 23:04:54,688][INFO]: Added DataFrame for file ../data/test/pagecounts-20071210-210000.gz to list\n",
      "100% (24 of 24) |########################| Elapsed Time: 0:00:00 ETA:  00:00:00\n",
      "[2018-05-22 23:04:54,726][INFO]: Union of all Spark DataFrames.\n",
      "[2018-05-22 23:04:56,825][INFO]: Spark DataFrame created\n",
      "[2018-05-22 23:04:58,535][INFO]: Dropping column \"reqbytes\" from DataFrame\n",
      "[2018-05-22 23:04:58,556][INFO]: Dropped column \"reqbytes\" from DataFrame\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "progressbar.streams.wrap_stderr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"merge-pagecounts\")\n",
    "sqlctx = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"lang\", StringType(), False),\n",
    "                     StructField(\"page\", StringType(), False),\n",
    "                     StructField(\"views\", IntegerType(), False),\n",
    "                     StructField(\"reqbytes\", IntegerType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(*dfs):\n",
    "    first, *_ = dfs  # Python 3.x, for 2.x you'll have to unpack manually\n",
    "    return first.sql_ctx.createDataFrame(\n",
    "        first.sql_ctx._sc.union([df.rdd for df in dfs]),\n",
    "        first.schema\n",
    "    )\n",
    "\n",
    "\n",
    "def date_parser(timestamp):\n",
    "    return datetime.datetime.strptime(timestamp, '%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathfile = \"../data/test/pagecounts-20071210-*.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files_count = len([f for f in glob.iglob(pathfile)])\n",
    "input_files = glob.iglob(pathfile)\n",
    "\n",
    "# input_files = [\"data/input/sorted_time/2007-12/pagecounts-20071210-000000.gz\",\n",
    "#                \"data/input/sorted_time/2007-12/pagecounts-20071210-010000.gz\"\n",
    "#                ]\n",
    "# input_files_count = len(input_files)\n",
    "\n",
    "logger.debug('input_files_count: {}'.format(input_files_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_files_count < 1:\n",
    "    logger.warn('No input files match: exiting')\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_files_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dfs = list()\n",
    "with progressbar.ProgressBar(max_value=input_files_count) as bar:\n",
    "    for input_file in input_files:\n",
    "        logger.debug('input_file: {}'.format(input_file))\n",
    "\n",
    "        timestamp = date_parser(os.path.basename(input_file)\n",
    "                                       .replace('pagecounts-','')\n",
    "                                       .replace('.gz',''))\n",
    "\n",
    "        logger.info('Processing file: {}'.format(input_file))\n",
    "        tmp_spark_df = sqlctx.read.csv(\n",
    "                            input_file,\n",
    "                            header=False,\n",
    "                            schema=schema,\n",
    "                            sep=' ')\n",
    "\n",
    "        tmp_spark_df = tmp_spark_df.withColumn(\"timestamp\", lit(timestamp))\n",
    "        list_dfs.append(tmp_spark_df)\n",
    "        del tmp_spark_df\n",
    "\n",
    "        logger.info('Added DataFrame for file {} to list'.format(input_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(list_dfs) >= 1, 'There should be at least one DataFrame'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(list_dfs) > 1:\n",
    "    logger.info('Union of all Spark DataFrames.')\n",
    "    df = unionAll(*list_dfs)\n",
    "    logger.info('Spark DataFrame created')\n",
    "else:\n",
    "    df = list_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Dropping column \"reqbytes\" from DataFrame')\n",
    "df = df.drop('reqbytes')\n",
    "logger.info('Dropped column \"reqbytes\" from DataFrame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lang', 'string'),\n",
       " ('page', 'string'),\n",
       " ('views', 'int'),\n",
       " ('timestamp', 'timestamp')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_daily_df = (df.select(['lang',\n",
    "                               'page',\n",
    "                               functions.date_format('timestamp','yyyy-MM-dd')\\\n",
    "                                        .alias('day'),\n",
    "                               'views'])\n",
    "                      .groupby(['lang','page','day'])\n",
    "                      .sum('views')\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+----------+\n",
      "|lang|                page|       day|sum(views)|\n",
      "+----+--------------------+----------+----------+\n",
      "|  en|Albert_Cardinal_D...|2007-12-10|         1|\n",
      "|  en| Albert_Caesar_Tocco|2007-12-10|         2|\n",
      "|  en|    Albert_C__Outler|2007-12-10|         1|\n",
      "|  en|Albert_Camus#Furt...|2007-12-10|         1|\n",
      "|  en|Albert_Camuscolum...|2007-12-10|         1|\n",
      "|  en|    Albert_Carnesale|2007-12-10|        12|\n",
      "|  en|        Albert_Buick|2007-12-10|         1|\n",
      "|  en|        Albert_Camus|2007-12-10|      2908|\n",
      "|  en|      Albert_Bunjaku|2007-12-10|         1|\n",
      "|  en|        Albert_Burgh|2007-12-10|         1|\n",
      "+----+--------------------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_daily_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_schema = StructType([StructField(\"lang\", StringType(), False),\n",
    "                         StructField(\"page\", StringType(), False),\n",
    "                         StructField(\"day\", StringType(), False),\n",
    "                         StructField(\"enc\", StringType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pandas as pd\n",
    "\n",
    "hour_to_letter = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O',\n",
    "                  'P','Q','R','S','T','U','V','W','X']\n",
    "\n",
    "@pandas_udf(new_schema, PandasUDFType.GROUPED_MAP)\n",
    "def concat_hours(x):\n",
    "    view_hours = x['hour'].tolist()\n",
    "    view_views = x['views'].tolist()\n",
    "\n",
    "    view_hours_letters = [hour_to_letter[h] for h in view_hours]\n",
    "\n",
    "    encoded_views = [l + str(h)\n",
    "                     for l, h in sorted(zip(view_hours_letters,view_views))]\n",
    "    encoded_views_string = ''.join(encoded_views)\n",
    "\n",
    "    # return pd.DataFrame({'page': x.page, 'lang': x.lang,'day': x.day, 'enc': encoded_views_string}, index=[x.index[0]])\n",
    "    return pd.DataFrame({'enc': x.page, 'day': x.lang, 'lang': x.day, 'page': encoded_views_string}, index=[x.index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions\n",
    "grouped_hours_df = (df.select(['lang',\n",
    "                               'page',\n",
    "                               functions.date_format('timestamp','yyyy-MM-dd').alias('day'), \n",
    "                               functions.hour('timestamp').alias('hour'), \n",
    "                               'views'\n",
    "                               ])\n",
    "                      .groupby(['lang','page','day'])\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_hours_df = (grouped_hours_df.apply(concat_hours)\n",
    "                                    .dropDuplicates()\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+--------------------+\n",
      "|lang|                page|       day|                 enc|\n",
      "+----+--------------------+----------+--------------------+\n",
      "|  en|   Albert_C._Ritchie|2007-12-10|            Q2S2V1W1|\n",
      "|  en|Albert_C._L._G._G...|2007-12-10|                  J1|\n",
      "|  en|      Albert_Calland|2007-12-10|        B1D3I1L1O1Q1|\n",
      "|  en|       Albert_Brooks|2007-12-10|           G16K11N13|\n",
      "|  en|    Albert_C1984amus|2007-12-10|                  V1|\n",
      "|  en| Albert_C._Wedemeyer|2007-12-10|                B1X1|\n",
      "|  en|       Albert_Chowne|2007-12-10|                  R1|\n",
      "|  en|        Albert_Camus|2007-12-10|A150B148C197D173E...|\n",
      "|  en|Albert_Cardinal_V...|2007-12-10|                  E1|\n",
      "|  en|    Albert_C__Barnes|2007-12-10|                  U1|\n",
      "|  en|Albert_Camus#Oppo...|2007-12-10|                  E1|\n",
      "|  en|     Albert_Calmette|2007-12-10|B1C1F1H1K1L1O1P2R...|\n",
      "|  en|Albert_Burnley_Botts|2007-12-10|                  M1|\n",
      "|  en|Albert_Camus_-_Wi...|2007-12-10|                  M1|\n",
      "|  en|      Albert_Campion|2007-12-10|B8C7D3E6H3I1J1K2Q...|\n",
      "|  en|   Albert_Brudzewski|2007-12-10|                  M2|\n",
      "|  en|Albert_Campbell_S...|2007-12-10|    E1F1H1R1S1T1W1X2|\n",
      "|  en|    Albert_C._Outler|2007-12-10|                H2T2|\n",
      "|  en|Albert_Cardozo%2C...|2007-12-10|            C1F1I1M1|\n",
      "|  en|Albert_CamusEarly...|2007-12-10|                  X1|\n",
      "+----+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_hours_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+----------+\n",
      "|lang|                page|       day|sum(views)|\n",
      "+----+--------------------+----------+----------+\n",
      "|  en|Albert_Cardinal_D...|2007-12-10|         1|\n",
      "|  en| Albert_Caesar_Tocco|2007-12-10|         2|\n",
      "|  en|    Albert_C__Outler|2007-12-10|         1|\n",
      "|  en|Albert_Camus#Furt...|2007-12-10|         1|\n",
      "|  en|Albert_Camuscolum...|2007-12-10|         1|\n",
      "|  en|    Albert_Carnesale|2007-12-10|        12|\n",
      "|  en|        Albert_Buick|2007-12-10|         1|\n",
      "|  en|        Albert_Camus|2007-12-10|      2908|\n",
      "|  en|      Albert_Bunjaku|2007-12-10|         1|\n",
      "|  en|        Albert_Burgh|2007-12-10|         1|\n",
      "+----+--------------------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_daily_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "daily = grouped_daily_df.select([col('lang').alias('daily_lang'),\n",
    "                                 col('page').alias('daily_page'),\n",
    "                                 col('day').alias('daily_day'),\n",
    "                                 col('sum(views)').alias('daily_sum_views'),                                 \n",
    "                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+---------------+\n",
      "|daily_lang|          daily_page| daily_day|daily_sum_views|\n",
      "+----------+--------------------+----------+---------------+\n",
      "|        en|Albert_Cardinal_D...|2007-12-10|              1|\n",
      "|        en| Albert_Caesar_Tocco|2007-12-10|              2|\n",
      "|        en|    Albert_C__Outler|2007-12-10|              1|\n",
      "|        en|Albert_Camus#Furt...|2007-12-10|              1|\n",
      "|        en|Albert_Camuscolum...|2007-12-10|              1|\n",
      "|        en|    Albert_Carnesale|2007-12-10|             12|\n",
      "|        en|        Albert_Buick|2007-12-10|              1|\n",
      "|        en|        Albert_Camus|2007-12-10|           2908|\n",
      "|        en|      Albert_Bunjaku|2007-12-10|              1|\n",
      "|        en|        Albert_Burgh|2007-12-10|              1|\n",
      "|        en|      Albert_Cadwell|2007-12-10|              1|\n",
      "|        en|       Albert_Caquot|2007-12-10|              8|\n",
      "|        en|     Albert_C._Field|2007-12-10|              2|\n",
      "|        en|Albert_Camus#Oppo...|2007-12-10|              1|\n",
      "|        en|         Albert_Chen|2007-12-10|              1|\n",
      "|        en|Albert_CamusNon-f...|2007-12-10|              1|\n",
      "|        en|   Albert_Brudzewski|2007-12-10|              2|\n",
      "|        en|      Albert_Celades|2007-12-10|             19|\n",
      "|        en|      Albert_Cashier|2007-12-10|             40|\n",
      "|        en|      Albert_Burbank|2007-12-10|              1|\n",
      "+----------+--------------------+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = [daily.daily_lang == grouped_hours_df.lang,\n",
    "        daily.daily_page == grouped_hours_df.page,\n",
    "        daily.daily_day == grouped_hours_df.day]\n",
    "final = daily.join(grouped_hours_df, cond).select(['daily_lang','daily_page','daily_day', 'daily_sum_views', 'enc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+---------------+--------------------+\n",
      "|daily_lang|          daily_page| daily_day|daily_sum_views|                 enc|\n",
      "+----------+--------------------+----------+---------------+--------------------+\n",
      "|        en|Albert_Cardinal_D...|2007-12-10|              1|                  Q1|\n",
      "|        en|    Albert_C__Outler|2007-12-10|              1|                  R1|\n",
      "|        en| Albert_Caesar_Tocco|2007-12-10|              2|                N1P1|\n",
      "|        en|Albert_Camus#Furt...|2007-12-10|              1|                  E1|\n",
      "|        en|Albert_Camuscolum...|2007-12-10|              1|                  J1|\n",
      "|        en|    Albert_Carnesale|2007-12-10|             12|          G4H5L1V1X1|\n",
      "|        en|        Albert_Buick|2007-12-10|              1|                  K1|\n",
      "|        en|      Albert_Bunjaku|2007-12-10|              1|                  A1|\n",
      "|        en|        Albert_Burgh|2007-12-10|              1|                  M1|\n",
      "|        en|        Albert_Camus|2007-12-10|           2908|A150B148C197D173E...|\n",
      "|        en|      Albert_Cadwell|2007-12-10|              1|                  D1|\n",
      "|        en|     Albert_C._Field|2007-12-10|              2|                G1O1|\n",
      "|        en|Albert_Camus#Oppo...|2007-12-10|              1|                  E1|\n",
      "|        en|       Albert_Caquot|2007-12-10|              8|        C1E1G1I1P3V1|\n",
      "|        en|         Albert_Chen|2007-12-10|              1|                  N1|\n",
      "|        en|Albert_CamusNon-f...|2007-12-10|              1|                  I1|\n",
      "|        en|   Albert_Brudzewski|2007-12-10|              2|                  M2|\n",
      "|        en|      Albert_Celades|2007-12-10|             19|A3B1J2K1N2P2Q1S2T...|\n",
      "|        en|      Albert_Cashier|2007-12-10|             40|A2B1D7E1H2K1L2M1O...|\n",
      "|        en|      Albert_Burbank|2007-12-10|              1|                  Q1|\n",
      "+----------+--------------------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
